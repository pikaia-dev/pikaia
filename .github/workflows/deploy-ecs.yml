name: Deploy Backend to AWS ECS

on:
  push:
    branches: ["main"]
    paths:
      - 'backend/**'
      - 'emails/**'
      - 'infra/**'
      - '.github/workflows/deploy-ecs.yml'
  workflow_dispatch:

concurrency:
  group: deploy-backend
  cancel-in-progress: true

permissions:
  contents: read
  id-token: write

env:
  AWS_REGION: ${{ vars.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ vars.AWS_ACCOUNT_ID }}

jobs:
  # Detect which paths changed to determine deployment strategy
  changes:
    runs-on: ubuntu-latest
    outputs:
      infra: ${{ steps.filter.outputs.infra }}
      backend: ${{ steps.filter.outputs.backend }}
      events: ${{ steps.filter.outputs.events }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            infra:
              - 'infra/**'
            backend:
              - 'backend/**'
              - 'emails/**'
            events:
              - 'backend/apps/events/**'

  deploy:
    needs: changes
    runs-on: ubuntu-latest
    environment: demo

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}

      - name: Lookup infrastructure outputs
        id: infra
        run: |
          set -euo pipefail

          # Construct derived values
          ECR_REPOSITORY="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/pikaia-backend"
          echo "ecr_repository=${ECR_REPOSITORY}" >> "$GITHUB_OUTPUT"

          # Helper function to lookup CloudFormation outputs with validation
          get_output() {
            local stack=$1
            local key=$2
            local value
            value=$(aws cloudformation describe-stacks \
              --stack-name "$stack" \
              --query "Stacks[0].Outputs[?OutputKey=='$key'].OutputValue" \
              --output text 2>/dev/null || echo "")
            if [[ -z "$value" ]]; then
              echo "::error::Required CloudFormation output '$key' not found in stack '$stack'. Run CDK deploy first." >&2
              return 1
            fi
            echo "$value"
          }

          # Lookup from PikaiaApp stack
          ECS_CLUSTER=$(get_output "PikaiaApp" "ClusterName")
          ECS_SERVICE=$(get_output "PikaiaApp" "ServiceName")
          ECS_TASK_DEFINITION=$(get_output "PikaiaApp" "TaskDefinitionFamily")
          ECS_SG=$(get_output "PikaiaApp" "EcsSecurityGroup")

          echo "ecs_cluster=${ECS_CLUSTER}" >> "$GITHUB_OUTPUT"
          echo "ecs_service=${ECS_SERVICE}" >> "$GITHUB_OUTPUT"
          echo "ecs_task_definition=${ECS_TASK_DEFINITION}" >> "$GITHUB_OUTPUT"
          echo "ecs_security_group=${ECS_SG}" >> "$GITHUB_OUTPUT"

          # Lookup from PikaiaNetwork stack
          PRIVATE_SUBNETS=$(get_output "PikaiaNetwork" "PrivateSubnets")
          echo "ecs_subnets=${PRIVATE_SUBNETS}" >> "$GITHUB_OUTPUT"

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      # Build and push Docker image when backend files change
      - name: Build and push Docker image
        if: needs.changes.outputs.backend == 'true'
        env:
          ECR_REPOSITORY: ${{ steps.infra.outputs.ecr_repository }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -f backend/Dockerfile -t "${ECR_REPOSITORY}:${IMAGE_TAG}" -t "${ECR_REPOSITORY}:latest" backend/
          docker push "${ECR_REPOSITORY}:${IMAGE_TAG}"
          docker push "${ECR_REPOSITORY}:latest"

      # Run CDK deploy when infra or events files change (updates Lambda, task definition, IAM, etc.)
      - name: Setup Python for CDK
        if: needs.changes.outputs.infra == 'true' || needs.changes.outputs.events == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.14'

      - name: Setup Node.js for CDK
        if: needs.changes.outputs.infra == 'true' || needs.changes.outputs.events == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install uv
        if: needs.changes.outputs.infra == 'true' || needs.changes.outputs.events == 'true'
        uses: astral-sh/setup-uv@v5
        with:
          version: "0.5.x"

      - name: Generate audit schema
        if: needs.changes.outputs.infra == 'true' || needs.changes.outputs.events == 'true'
        working-directory: backend
        run: |
          uv sync
          uv run python manage.py generate_audit_schema

      - name: Install CDK and deploy infrastructure
        if: needs.changes.outputs.infra == 'true' || needs.changes.outputs.events == 'true'
        working-directory: infra
        env:
          # Build CDK context from environment variables
          # Required: AVAILABILITY_ZONES prevents CDK AZ lookups that cause subnet replacement
          # Optional: CERTIFICATE_ARN and APP_DOMAIN_NAME for HTTPS deployments
          CDK_CONTEXT: >-
            --context account=${{ env.AWS_ACCOUNT_ID }}
            --context region=${{ env.AWS_REGION }}
            --context availability_zones=${{ vars.AVAILABILITY_ZONES }}
            ${{ vars.CERTIFICATE_ARN && format('--context certificate_arn={0}', vars.CERTIFICATE_ARN) || '' }}
            ${{ vars.APP_DOMAIN_NAME && format('--context app_domain={0}', vars.APP_DOMAIN_NAME) || '' }}
        run: |
          npm install -g aws-cdk
          uv sync
          # Deploy PikaiaApp exclusively first to handle cross-stack export changes
          # (prevents "Cannot delete export" errors when moving resources between stacks)
          uv run cdk deploy PikaiaApp --exclusively --require-approval never $CDK_CONTEXT
          # Then deploy all stacks including PikaiaNetwork
          uv run cdk deploy PikaiaNetwork PikaiaMedia PikaiaApp PikaiaEvents PikaiaObservability --require-approval never $CDK_CONTEXT

      # Update ECS task definition when backend changes but not infra
      # (CDK deploy already handles this when infra changes)
      - name: Prepare and register new task definition revision
        if: needs.changes.outputs.backend == 'true' && needs.changes.outputs.infra != 'true'
        id: td
        env:
          ECR_REPOSITORY: ${{ steps.infra.outputs.ecr_repository }}
          ECS_TASK_DEFINITION: ${{ steps.infra.outputs.ecs_task_definition }}
        run: |
          set -euo pipefail

          NEW_IMAGE="${ECR_REPOSITORY}:${{ github.sha }}"

          aws ecs describe-task-definition \
            --task-definition "${ECS_TASK_DEFINITION}" \
            --query 'taskDefinition' \
            --output json > td.json

          jq --arg IMAGE "${NEW_IMAGE}" '
            .containerDefinitions |= map(.image=$IMAGE)
            | del(.revision, .status, .taskDefinitionArn, .requiresAttributes, .compatibilities, .registeredAt, .registeredBy)
          ' td.json > td-updated.json

          aws ecs register-task-definition \
            --cli-input-json file://td-updated.json \
            --output json > td-registered.json

          NEW_TD_ARN=$(jq -r '.taskDefinition.taskDefinitionArn' td-registered.json)
          echo "taskDefinitionArn=${NEW_TD_ARN}" >> "$GITHUB_OUTPUT"

      - name: Update ECS service (backend-only changes)
        if: needs.changes.outputs.backend == 'true' && needs.changes.outputs.infra != 'true'
        env:
          ECS_CLUSTER: ${{ steps.infra.outputs.ecs_cluster }}
          ECS_SERVICE: ${{ steps.infra.outputs.ecs_service }}
        run: |
          aws ecs update-service \
            --cluster "${ECS_CLUSTER}" \
            --service "${ECS_SERVICE}" \
            --task-definition "${{ steps.td.outputs.taskDefinitionArn }}" \
            --force-new-deployment \
            --output json > service-update.json

          aws ecs wait services-stable \
            --cluster "${ECS_CLUSTER}" \
            --services "${ECS_SERVICE}"

      # Force new deployment after CDK to pick up changes
      - name: Force new deployment (infra/events changes)
        if: needs.changes.outputs.infra == 'true' || needs.changes.outputs.events == 'true'
        env:
          ECS_CLUSTER: ${{ steps.infra.outputs.ecs_cluster }}
          ECS_SERVICE: ${{ steps.infra.outputs.ecs_service }}
        run: |
          aws ecs update-service \
            --cluster "${ECS_CLUSTER}" \
            --service "${ECS_SERVICE}" \
            --force-new-deployment \
            --output json > service-update.json

          aws ecs wait services-stable \
            --cluster "${ECS_CLUSTER}" \
            --services "${ECS_SERVICE}"

      # Always run migrations
      - name: Get task definition for migrations
        id: get-td
        env:
          ECS_CLUSTER: ${{ steps.infra.outputs.ecs_cluster }}
          ECS_SERVICE: ${{ steps.infra.outputs.ecs_service }}
        run: |
          TD_ARN=$(aws ecs describe-services \
            --cluster "${ECS_CLUSTER}" \
            --services "${ECS_SERVICE}" \
            --query 'services[0].taskDefinition' \
            --output text)
          echo "taskDefinitionArn=${TD_ARN}" >> "$GITHUB_OUTPUT"

          MIGRATE_CONTAINER=$(aws ecs describe-task-definition \
            --task-definition "${TD_ARN}" \
            --query 'taskDefinition.containerDefinitions[0].name' \
            --output text)
          echo "migrateContainer=${MIGRATE_CONTAINER}" >> "$GITHUB_OUTPUT"

      - name: Create cache table
        env:
          ECS_CLUSTER: ${{ steps.infra.outputs.ecs_cluster }}
          ECS_SUBNETS: ${{ steps.infra.outputs.ecs_subnets }}
          ECS_SECURITY_GROUP: ${{ steps.infra.outputs.ecs_security_group }}
        run: |
          set -euo pipefail

          TASK_ARN=$(aws ecs run-task \
            --cluster "${ECS_CLUSTER}" \
            --launch-type FARGATE \
            --task-definition "${{ steps.get-td.outputs.taskDefinitionArn }}" \
            --network-configuration "awsvpcConfiguration={subnets=[${ECS_SUBNETS}],securityGroups=[${ECS_SECURITY_GROUP}],assignPublicIp=DISABLED}" \
            --count 1 \
            --overrides "{\"containerOverrides\":[{\"name\":\"${{ steps.get-td.outputs.migrateContainer }}\",\"command\":[\"python\",\"manage.py\",\"createcachetable\"]}]}" \
            --query 'tasks[0].taskArn' \
            --output text)

          if [ -z "$TASK_ARN" ] || [ "$TASK_ARN" = "None" ]; then
            echo "::error::Failed to launch cache table task"
            exit 1
          fi

          echo "Waiting for cache table task to complete: ${TASK_ARN}"
          aws ecs wait tasks-stopped --cluster "${ECS_CLUSTER}" --tasks "${TASK_ARN}"

          EXIT_CODE=$(aws ecs describe-tasks \
            --cluster "${ECS_CLUSTER}" \
            --tasks "${TASK_ARN}" \
            --query 'tasks[0].containers[0].exitCode' \
            --output text)

          if [ -z "$EXIT_CODE" ] || [ "$EXIT_CODE" = "None" ]; then
            echo "::error::Create cache table task did not complete properly (no exit code)"
            exit 1
          fi

          if [ "$EXIT_CODE" != "0" ]; then
            echo "::error::Create cache table task failed with exit code ${EXIT_CODE}"
            exit 1
          fi

      - name: Run Django migrations
        env:
          ECS_CLUSTER: ${{ steps.infra.outputs.ecs_cluster }}
          ECS_SUBNETS: ${{ steps.infra.outputs.ecs_subnets }}
          ECS_SECURITY_GROUP: ${{ steps.infra.outputs.ecs_security_group }}
        run: |
          set -euo pipefail

          TASK_ARN=$(aws ecs run-task \
            --cluster "${ECS_CLUSTER}" \
            --launch-type FARGATE \
            --task-definition "${{ steps.get-td.outputs.taskDefinitionArn }}" \
            --network-configuration "awsvpcConfiguration={subnets=[${ECS_SUBNETS}],securityGroups=[${ECS_SECURITY_GROUP}],assignPublicIp=DISABLED}" \
            --count 1 \
            --overrides "{\"containerOverrides\":[{\"name\":\"${{ steps.get-td.outputs.migrateContainer }}\",\"command\":[\"python\",\"manage.py\",\"migrate\"]}]}" \
            --query 'tasks[0].taskArn' \
            --output text)

          if [ -z "$TASK_ARN" ] || [ "$TASK_ARN" = "None" ]; then
            echo "::error::Failed to launch migration task"
            exit 1
          fi

          echo "Waiting for migration task to complete: ${TASK_ARN}"
          aws ecs wait tasks-stopped --cluster "${ECS_CLUSTER}" --tasks "${TASK_ARN}"

          EXIT_CODE=$(aws ecs describe-tasks \
            --cluster "${ECS_CLUSTER}" \
            --tasks "${TASK_ARN}" \
            --query 'tasks[0].containers[0].exitCode' \
            --output text)

          if [ -z "$EXIT_CODE" ] || [ "$EXIT_CODE" = "None" ]; then
            echo "::error::Django migration task did not complete properly (no exit code)"
            exit 1
          fi

          if [ "$EXIT_CODE" != "0" ]; then
            echo "::error::Django migration task failed with exit code ${EXIT_CODE}"
            exit 1
          fi
